{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T11:39:46.241088Z","iopub.execute_input":"2024-05-05T11:39:46.241671Z","iopub.status.idle":"2024-05-05T11:39:47.264367Z","shell.execute_reply.started":"2024-05-05T11:39:46.241643Z","shell.execute_reply":"2024-05-05T11:39:47.263459Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:40:08.167825Z","iopub.execute_input":"2024-05-05T11:40:08.168976Z","iopub.status.idle":"2024-05-05T11:40:11.873387Z","shell.execute_reply.started":"2024-05-05T11:40:08.168940Z","shell.execute_reply":"2024-05-05T11:40:11.872377Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install stormtrooper[setfit]","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:40:53.355387Z","iopub.execute_input":"2024-05-05T11:40:53.355892Z","iopub.status.idle":"2024-05-05T11:41:43.151439Z","shell.execute_reply.started":"2024-05-05T11:40:53.355861Z","shell.execute_reply":"2024-05-05T11:41:43.150243Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bert-for-tf2\n  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m656.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py-params>=0.9.6 (from bert-for-tf2)\n  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting params-flow>=0.8.0 (from bert-for-tf2)\n  Downloading params-flow-0.8.2.tar.gz (22 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.1)\nBuilding wheels for collected packages: bert-for-tf2, params-flow, py-params\n  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30509 sha256=5eab9926520a51108ad9e9aa3de2db804ab2622a49e55a760f20fe54ee10a1dc\n  Stored in directory: /root/.cache/pip/wheels/d8/da/50/126d7b8416d9a0e6bf876935c2219a71e72a6529c25e150c56\n  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19454 sha256=eae42825f51143831a1cf29ff8bcdca256fa7f7a6933a74487ebcd990b322417\n  Stored in directory: /root/.cache/pip/wheels/97/a8/d0/f7419404174976a2686bb98b5c30df01cc71445415f32db9e6\n  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7891 sha256=db7c10aa6ed668a8c2389cc8ef40346a428fea615c5175c62d113add4aa708b4\n  Stored in directory: /root/.cache/pip/wheels/69/c8/b3/92666cff9fb312bc3473eaa6b396695b89a7b3e31e90876819\nSuccessfully built bert-for-tf2 params-flow py-params\nInstalling collected packages: py-params, params-flow, bert-for-tf2\nSuccessfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting stormtrooper[setfit]\n  Downloading stormtrooper-0.4.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from stormtrooper[setfit]) (3.9.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from stormtrooper[setfit]) (1.26.4)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stormtrooper[setfit]) (1.2.2)\nCollecting thefuzz<0.19.0,>=0.18.0 (from stormtrooper[setfit])\n  Downloading thefuzz-0.18.0.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.60.0 in /opt/conda/lib/python3.10/site-packages (from stormtrooper[setfit]) (4.66.1)\nRequirement already satisfied: transformers<5.0.0,>=4.25.0 in /opt/conda/lib/python3.10/site-packages (from stormtrooper[setfit]) (4.39.3)\nRequirement already satisfied: datasets<3.0.0,>=2.14.0 in /opt/conda/lib/python3.10/site-packages (from stormtrooper[setfit]) (2.18.0)\nCollecting setfit<0.8.0,>=0.7.0 (from stormtrooper[setfit])\n  Downloading setfit-0.7.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->stormtrooper[setfit]) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->stormtrooper[setfit]) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->stormtrooper[setfit]) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->stormtrooper[setfit]) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->stormtrooper[setfit]) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->stormtrooper[setfit]) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2.31.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (6.0.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.0->stormtrooper[setfit]) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.0->stormtrooper[setfit]) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.0->stormtrooper[setfit]) (3.2.0)\nCollecting sentence-transformers>=2.2.1 (from setfit<0.8.0,>=0.7.0->stormtrooper[setfit])\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nCollecting evaluate>=0.3.0 (from setfit<0.8.0,>=0.7.0->stormtrooper[setfit])\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.25.0->stormtrooper[setfit]) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.25.0->stormtrooper[setfit]) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.25.0->stormtrooper[setfit]) (0.4.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2024.2.2)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (2.1.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (9.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets<3.0.0,>=2.14.0->stormtrooper[setfit]) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.2.1->setfit<0.8.0,>=0.7.0->stormtrooper[setfit]) (1.3.0)\nDownloading setfit-0.7.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stormtrooper-0.4.1-py3-none-any.whl (22 kB)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: thefuzz\n  Building wheel for thefuzz (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for thefuzz: filename=thefuzz-0.18.0-py2.py3-none-any.whl size=18199 sha256=3eb6870468fd7f3c682833fb8ab9a657c8d91e02c17ece588302380278d88ee3\n  Stored in directory: /root/.cache/pip/wheels/8f/5b/ba/9b1a261ddd5216f10d8c6b14007d5db4b1aa16a4556d682031\nSuccessfully built thefuzz\nInstalling collected packages: thefuzz, stormtrooper, sentence-transformers, evaluate, setfit\nSuccessfully installed evaluate-0.4.2 sentence-transformers-2.7.0 setfit-0.7.0 stormtrooper-0.4.1 thefuzz-0.18.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport math\nfrom bs4 import BeautifulSoup  ## For cleanin the data\nimport random\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:41:43.153476Z","iopub.execute_input":"2024-05-05T11:41:43.153816Z","iopub.status.idle":"2024-05-05T11:41:57.389610Z","shell.execute_reply.started":"2024-05-05T11:41:43.153787Z","shell.execute_reply":"2024-05-05T11:41:57.388641Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-05 11:41:45.457193: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-05 11:41:45.457318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-05 11:41:45.621679: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Processing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\ndata.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:05.651555Z","iopub.execute_input":"2024-05-05T11:42:05.652528Z","iopub.status.idle":"2024-05-05T11:42:20.440442Z","shell.execute_reply.started":"2024-05-05T11:42:05.652495Z","shell.execute_reply":"2024-05-05T11:42:20.439446Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   label        time                          date     query       username  \\\n0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n\n                                                text  \n0  is upset that he can't update his Facebook by ...  \n1  @Kenichan I dived many times for the ball. Man...  \n2    my whole body feels itchy and like its on fire   \n3  @nationwideclass no, it's not behaving at all....  \n4                      @Kwesidei not the whole crew   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>time</th>\n      <th>date</th>\n      <th>query</th>\n      <th>username</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811372</td>\n      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>joy_wolf</td>\n      <td>@Kwesidei not the whole crew</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:21.910835Z","iopub.execute_input":"2024-05-05T11:42:21.911846Z","iopub.status.idle":"2024-05-05T11:42:21.943005Z","shell.execute_reply.started":"2024-05-05T11:42:21.911807Z","shell.execute_reply":"2024-05-05T11:42:21.942056Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label\n4    800000\n0    799999\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.drop([\"time\",\"date\",\"query\",\"username\"], axis= 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:24.098706Z","iopub.execute_input":"2024-05-05T11:42:24.099050Z","iopub.status.idle":"2024-05-05T11:42:24.140502Z","shell.execute_reply.started":"2024-05-05T11:42:24.099025Z","shell.execute_reply":"2024-05-05T11:42:24.139593Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data.columns=[ 'sentiment','tweet']\ndata['sentiment'] = data['sentiment'].replace(4,1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:24.551764Z","iopub.execute_input":"2024-05-05T11:42:24.552155Z","iopub.status.idle":"2024-05-05T11:42:24.571013Z","shell.execute_reply.started":"2024-05-05T11:42:24.552125Z","shell.execute_reply":"2024-05-05T11:42:24.570096Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   sentiment                                              tweet\n0          0  is upset that he can't update his Facebook by ...\n1          0  @Kenichan I dived many times for the ball. Man...\n2          0    my whole body feels itchy and like its on fire \n3          0  @nationwideclass no, it's not behaving at all....\n4          0                      @Kwesidei not the whole crew ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@Kwesidei not the whole crew</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Instead of working on complete data - create small fraction for test and train\n\n# data_train = data.sample(n=100000, random_state=42)\ndata_train = data.sample(frac=0.15, random_state=42)\nsubset2 = data.drop(data_train.index)\ndata_test = subset2.sample(frac=0.05, random_state=42)\n\nprint(f'New sampled train data:', {data_train.shape})\nprint(f'New sampled train data:', {data_test.shape})","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:27.277122Z","iopub.execute_input":"2024-05-05T11:42:27.277470Z","iopub.status.idle":"2024-05-05T11:42:27.489225Z","shell.execute_reply.started":"2024-05-05T11:42:27.277443Z","shell.execute_reply":"2024-05-05T11:42:27.488254Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"New sampled train data: {(240000, 2)}\nNew sampled train data: {(68000, 2)}\n","output_type":"stream"}]},{"cell_type":"code","source":"data_train.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:30.364469Z","iopub.execute_input":"2024-05-05T11:42:30.364845Z","iopub.status.idle":"2024-05-05T11:42:30.374926Z","shell.execute_reply.started":"2024-05-05T11:42:30.364816Z","shell.execute_reply":"2024-05-05T11:42:30.373879Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"sentiment\n1    120175\n0    119825\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data_test.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:30.952242Z","iopub.execute_input":"2024-05-05T11:42:30.953017Z","iopub.status.idle":"2024-05-05T11:42:30.960475Z","shell.execute_reply.started":"2024-05-05T11:42:30.952988Z","shell.execute_reply":"2024-05-05T11:42:30.959612Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"sentiment\n1    34082\n0    33918\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"def clean_tweet(tweet):\n    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n    tweet = re.sub(r\"https?://\\S+|www\\.\\S+\",\" \",tweet) # Removing URLS\n    tweet = re.sub(r\"@[A-Za-z0-9]+\",\" \",tweet) # @texts\n    tweet = re.sub(r\"https?://[A-Za-z0-9]+\",\" \",tweet) # Removing html tags\n    tweet = re.sub(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\",\" \",tweet) # Removing html tags\n    tweet = re.sub(r\"[^\\w\\s]\", \" \", tweet) # Removing Punctuation\n    tweet = re.sub(r\" +\", \" \", tweet) # Removing repeatition of anything\n    tweet = re.sub(r\"[^a-zA-Z.!?']\", \" \", tweet) # Removing non-letter words\n    tweet = re.sub(r\"\\s+\", \" \", tweet).strip() # Removing whitespace\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:33.409552Z","iopub.execute_input":"2024-05-05T11:42:33.410622Z","iopub.status.idle":"2024-05-05T11:42:33.418700Z","shell.execute_reply.started":"2024-05-05T11:42:33.410563Z","shell.execute_reply":"2024-05-05T11:42:33.417633Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data_clean = [clean_tweet(tweet) for tweet in data_train.tweet] ## Cleaned Train data","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:42:36.436597Z","iopub.execute_input":"2024-05-05T11:42:36.436961Z","iopub.status.idle":"2024-05-05T11:43:26.897420Z","shell.execute_reply.started":"2024-05-05T11:42:36.436935Z","shell.execute_reply":"2024-05-05T11:43:26.896630Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4071081272.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n","output_type":"stream"}]},{"cell_type":"code","source":"data_test_clean = [clean_tweet(tweet) for tweet in data_test.tweet]","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:43:49.024571Z","iopub.execute_input":"2024-05-05T11:43:49.025252Z","iopub.status.idle":"2024-05-05T11:44:03.258400Z","shell.execute_reply.started":"2024-05-05T11:43:49.025221Z","shell.execute_reply":"2024-05-05T11:44:03.257358Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4071081272.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"FullTokenization = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\", trainable = False)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = FullTokenization(vocab_file,do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:11:00.252996Z","iopub.execute_input":"2024-05-04T19:11:00.253854Z","iopub.status.idle":"2024-05-04T19:11:13.021345Z","shell.execute_reply.started":"2024-05-04T19:11:00.253817Z","shell.execute_reply":"2024-05-04T19:11:13.020495Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.tokenize(\"This is such a weird place to LIVEEEE\")) ## Convert to token\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"This is such a weird place to LIVEEEE\"))) ## Token to IDS\ntokenizer.convert_ids_to_tokens([2023, 2003, 2107, 1037, 6881, 2173, 2000, 2444, 4402, 2063]) ## Is to tokens","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:11:15.800101Z","iopub.execute_input":"2024-05-04T19:11:15.801092Z","iopub.status.idle":"2024-05-04T19:11:15.810563Z","shell.execute_reply.started":"2024-05-04T19:11:15.801044Z","shell.execute_reply":"2024-05-04T19:11:15.809563Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"['this', 'is', 'such', 'a', 'weird', 'place', 'to', 'live', '##ee', '##e']\n[2023, 2003, 2107, 1037, 6881, 2173, 2000, 2444, 4402, 2063]\n","output_type":"stream"},{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"['this', 'is', 'such', 'a', 'weird', 'place', 'to', 'live', '##ee', '##e']"},"metadata":{}}]},{"cell_type":"code","source":"## We will use first sentence  for BERT inputs -  CLS is for classification\ndef encode_sentence(sent):\n    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:11:18.598388Z","iopub.execute_input":"2024-05-04T19:11:18.598755Z","iopub.status.idle":"2024-05-04T19:11:18.603520Z","shell.execute_reply.started":"2024-05-04T19:11:18.598725Z","shell.execute_reply":"2024-05-04T19:11:18.602488Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"data_inputs = [encode_sentence(tweet) for tweet in data_clean]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:11:20.838844Z","iopub.execute_input":"2024-05-04T19:11:20.839443Z","iopub.status.idle":"2024-05-04T19:12:33.059025Z","shell.execute_reply.started":"2024-05-04T19:11:20.839413Z","shell.execute_reply":"2024-05-04T19:12:33.058174Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"data_inputs[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:12:40.498174Z","iopub.execute_input":"2024-05-04T19:12:40.498906Z","iopub.status.idle":"2024-05-04T19:12:40.505186Z","shell.execute_reply.started":"2024-05-04T19:12:40.498876Z","shell.execute_reply":"2024-05-04T19:12:40.504304Z"},"trusted":true},"execution_count":134,"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'my',\n 'poor',\n 'little',\n 'dump',\n '##ling',\n 'in',\n 'holm',\n '##del',\n 'vi',\n '##ds',\n 'he',\n 'was',\n 'really',\n 'trying',\n 'hope',\n 'he',\n 'don',\n '##t',\n 'try',\n 'to',\n 'hard',\n 'tonight',\n 'xx',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Data Creation\nWe need to create 3 different inputs  for each sentence","metadata":{}},{"cell_type":"code","source":"def get_ids(tokens):\n    return tokenizer.convert_tokens_to_ids(tokens)\n\n## Mask Padding\ndef get_mask(tokens):\n    return np.char.not_equal(tokens,\"[PAD]\").astype(int)\n\ndef get_segments(tokens):\n    seg_ids = []\n    current_seg_id = 0\n    for tok in tokens:\n        seg_ids.append(current_seg_id)\n        if  tok == '[SEP]':\n            current_seg_id = 1-current_seg_id\n    return seg_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:16:45.216939Z","iopub.execute_input":"2024-05-04T19:16:45.217887Z","iopub.status.idle":"2024-05-04T19:16:45.223921Z","shell.execute_reply.started":"2024-05-04T19:16:45.217851Z","shell.execute_reply":"2024-05-04T19:16:45.222866Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"data_labels = list(data_train.sentiment)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:12:45.683374Z","iopub.execute_input":"2024-05-04T19:12:45.683739Z","iopub.status.idle":"2024-05-04T19:12:45.718228Z","shell.execute_reply.started":"2024-05-04T19:12:45.683709Z","shell.execute_reply":"2024-05-04T19:12:45.717256Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"data_with_len = [[sent,data_labels[i],len(sent)] for i,sent in enumerate(data_inputs)]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:12:48.622142Z","iopub.execute_input":"2024-05-04T19:12:48.622883Z","iopub.status.idle":"2024-05-04T19:12:48.760579Z","shell.execute_reply.started":"2024-05-04T19:12:48.622850Z","shell.execute_reply":"2024-05-04T19:12:48.759547Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"data_with_len[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:12:51.152712Z","iopub.execute_input":"2024-05-04T19:12:51.153115Z","iopub.status.idle":"2024-05-04T19:12:51.160450Z","shell.execute_reply.started":"2024-05-04T19:12:51.153081Z","shell.execute_reply":"2024-05-04T19:12:51.159301Z"},"trusted":true},"execution_count":138,"outputs":[{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"[['[CLS]',\n  'my',\n  'poor',\n  'little',\n  'dump',\n  '##ling',\n  'in',\n  'holm',\n  '##del',\n  'vi',\n  '##ds',\n  'he',\n  'was',\n  'really',\n  'trying',\n  'hope',\n  'he',\n  'don',\n  '##t',\n  'try',\n  'to',\n  'hard',\n  'tonight',\n  'xx',\n  '[SEP]'],\n 0,\n 25]"},"metadata":{}}]},{"cell_type":"code","source":"for i,sent_lab in enumerate(data_with_len):\n    if i<3:\n        if sent_lab[2]>7:\n            print(sent_lab)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:13:38.382078Z","iopub.execute_input":"2024-05-04T19:13:38.383029Z","iopub.status.idle":"2024-05-04T19:13:38.388261Z","shell.execute_reply.started":"2024-05-04T19:13:38.382994Z","shell.execute_reply":"2024-05-04T19:13:38.387218Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"[['[CLS]', 'my', 'poor', 'little', 'dump', '##ling', 'in', 'holm', '##del', 'vi', '##ds', 'he', 'was', 'really', 'trying', 'hope', 'he', 'don', '##t', 'try', 'to', 'hard', 'tonight', 'xx', '[SEP]'], 0, 25]\n","output_type":"stream"}]},{"cell_type":"code","source":"random.shuffle(data_with_len)\ndata_with_len.sort(key = lambda x: x[2])\n# sorted_all = [(sent_lab[0],sent_lab[1]) for sent_lab in data_with_len if sent_lab[2]>7]\n\nsorted_all = [([get_ids(sent_lab[0]), \n                get_mask(sent_lab[0]), \n                get_segments(sent_lab[0])], sent_lab[1])\n             for sent_lab in data_with_len if sent_lab[2]>7]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:16:49.928816Z","iopub.execute_input":"2024-05-04T19:16:49.929183Z","iopub.status.idle":"2024-05-04T19:16:57.680643Z","shell.execute_reply.started":"2024-05-04T19:16:49.929153Z","shell.execute_reply":"2024-05-04T19:16:57.679574Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all, \n                                              output_types = (tf.int32, tf.int32))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:17:35.472711Z","iopub.execute_input":"2024-05-04T19:17:35.473619Z","iopub.status.idle":"2024-05-04T19:17:35.507344Z","shell.execute_reply.started":"2024-05-04T19:17:35.473584Z","shell.execute_reply":"2024-05-04T19:17:35.506438Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"next(iter(all_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:17:37.913409Z","iopub.execute_input":"2024-05-04T19:17:37.914260Z","iopub.status.idle":"2024-05-04T19:17:37.948562Z","shell.execute_reply.started":"2024-05-04T19:17:37.914212Z","shell.execute_reply":"2024-05-04T19:17:37.947574Z"},"trusted":true},"execution_count":146,"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n array([[ 101, 2434, 1045, 1049, 2204, 2205, 4283,  102],\n        [   1,    1,    1,    1,    1,    1,    1,    1],\n        [   0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)>,\n <tf.Tensor: shape=(), dtype=int32, numpy=1>)"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 32\n# all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes = ( (None, ), () ))\nall_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes = ( (3,None), () ))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:59:47.391638Z","iopub.execute_input":"2024-05-04T17:59:47.392398Z","iopub.status.idle":"2024-05-04T17:59:47.400963Z","shell.execute_reply.started":"2024-05-04T17:59:47.392367Z","shell.execute_reply":"2024-05-04T17:59:47.400151Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# next(iter(all_batched))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:59:57.701357Z","iopub.execute_input":"2024-05-04T17:59:57.702191Z","iopub.status.idle":"2024-05-04T17:59:57.740874Z","shell.execute_reply.started":"2024-05-04T17:59:57.702158Z","shell.execute_reply":"2024-05-04T17:59:57.740003Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n array([[27571,  9541, 17139,  9152, 28954, 13900, 10474,  3122],\n        [ 7929,  2292,  1055,  2079,  2023,  2057, 19987,  2063],\n        [ 4283,  9212,  2072,  8840,  2140,  2200,  3835,  5449],\n        [ 2748,  2045,  2003,  1037,  2607,  2011,  2008,  2171],\n        [ 1045,  2123,  1056,  2228, 14106, 20627,  2030,  3571],\n        [ 2672,  1037,  2210,  4763,  2158,  2007,  1037,  8691],\n        [ 2129,  1045,  2777,  2115,  2388,  3084,  2033,  6517],\n        [ 1062,  1045,  4478,  1050,  2965,  9119, 23746,  8516],\n        [ 2039,  2025,  2061,  3835,  1998,  2220, 20323,  2082],\n        [ 4283,  2106,  2008,  2021,  2009,  2134,  1056,  2147],\n        [ 2123,  1056,  5390,  2009,  1055,  7929,  3081,  2078],\n        [ 4067,  2017,  2004, 17207,  2056,  1045,  2734,  2009],\n        [ 3666,  1996,  4564,  2059,  1996,  8692,  3185,  2982],\n        [23746,  2015,  2061, 10140,  1998,  2061,  2003,  2016],\n        [10047,  5962, 19810,  1045, 28667,  8462,  4859,  2017],\n        [ 7110,  1056, 17056,  4902,  1045,  1049,  3374, 13813],\n        [ 1045,  3984, 19902,  2003, 26478, 22974,  3560,  4656],\n        [ 2073,  1996,  6616,  2024,  2026,  2132, 19093,  6682],\n        [ 2467,  2245,  2009,  2001,  1996,  2060,  2126,  2105],\n        [ 3427,  8011,  2033,  2000,  3109,  2651,  3492,  2204],\n        [ 8117,  2187,  2000,  2147,  1998,  2059,  2733,  6209],\n        [ 2003,  5305,  2153,  1045,  5223,  2023, 13608,  4633],\n        [ 3599,  4165,  2066,  2017,  2128,  9107,  2115,  3944],\n        [ 1045, 20160,  2080,  2021, 22320,  2428,  3492,  2182],\n        [ 2758,  1037,  3148,  2860, 23848,  7170, 12835, 22110],\n        [ 2551,  2323,  2022,  7917,  2006,  2420,  2066,  2651],\n        [ 1045,  2113,  2008,  1055,  1996,  2028,  1045,  2215],\n        [ 2053,  2025,  1037,  3453,  1999,  2023,  2553, 22975],\n        [ 1045,  2074,  3427,  2008,  2792,  2153,  2197,  2305],\n        [ 2017,  2020,  5791,  4147,  2008,  6598,  2197,  2305],\n        [ 2204,  2851,  1045,  1049,  2145,  3103,  8022,  2102],\n        [ 3398,  4315,  3674,  2595,  2121, 14916,  3372,  9459]],\n       dtype=int32)>,\n <tf.Tensor: shape=(32,), dtype=int32, numpy=\n array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n        1, 1, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32)>)"},"metadata":{}}]},{"cell_type":"code","source":"# NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE)\n# NB_BATCHES_TEST = NB_BATCHES//10\n\n# all_batched.shuffle(NB_BATCHES)\n\n# ## Test and Train dataset create\n# test_dataset = all_batched.take(NB_BATCHES_TEST)\n# train_dataset = all_batched.skip(NB_BATCHES_TEST)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:06:02.446721Z","iopub.execute_input":"2024-05-04T18:06:02.447418Z","iopub.status.idle":"2024-05-04T18:06:02.466025Z","shell.execute_reply.started":"2024-05-04T18:06:02.447383Z","shell.execute_reply":"2024-05-04T18:06:02.465278Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# print(test_dataset)\n# train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:06:15.589542Z","iopub.execute_input":"2024-05-04T18:06:15.590276Z","iopub.status.idle":"2024-05-04T18:06:15.596932Z","shell.execute_reply.started":"2024-05-04T18:06:15.590242Z","shell.execute_reply":"2024-05-04T18:06:15.595985Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"<_TakeDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n","output_type":"stream"},{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"<_SkipDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Train \n In BERT, we follow classic method\n  - Sentence Tokenization\n  - Sentence Embeddings -  Use pretrained model to do this.\n  - Build model - any CNN\n  ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRY LLM classifier","metadata":{}},{"cell_type":"code","source":"data_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:45:28.529555Z","iopub.execute_input":"2024-05-05T11:45:28.529978Z","iopub.status.idle":"2024-05-05T11:45:28.540459Z","shell.execute_reply.started":"2024-05-05T11:45:28.529950Z","shell.execute_reply":"2024-05-05T11:45:28.539280Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"        sentiment                                              tweet\n541200          0  @Nkluvr4eva My poor little dumpling  In Holmde...\n750             0  I'm off too bed. I gotta wake up hella early t...\n766711          0  I havent been able to listen to it yet  My spe...\n285055          0  now remembers why solving a relatively big equ...\n705995          0                           Ate too much, feel sick ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>541200</th>\n      <td>0</td>\n      <td>@Nkluvr4eva My poor little dumpling  In Holmde...</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>0</td>\n      <td>I'm off too bed. I gotta wake up hella early t...</td>\n    </tr>\n    <tr>\n      <th>766711</th>\n      <td>0</td>\n      <td>I havent been able to listen to it yet  My spe...</td>\n    </tr>\n    <tr>\n      <th>285055</th>\n      <td>0</td>\n      <td>now remembers why solving a relatively big equ...</td>\n    </tr>\n    <tr>\n      <th>705995</th>\n      <td>0</td>\n      <td>Ate too much, feel sick</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train['cleaned_tweet'] = [clean_tweet(tweet) for tweet in data_train.tweet] ## Cleaned Train data\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:51:01.733101Z","iopub.execute_input":"2024-05-05T11:51:01.733467Z","iopub.status.idle":"2024-05-05T11:51:01.746650Z","shell.execute_reply.started":"2024-05-05T11:51:01.733440Z","shell.execute_reply":"2024-05-05T11:51:01.745608Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"        sentiment                                              tweet  \\\n541200          0  @Nkluvr4eva My poor little dumpling  In Holmde...   \n750             0  I'm off too bed. I gotta wake up hella early t...   \n766711          0  I havent been able to listen to it yet  My spe...   \n285055          0  now remembers why solving a relatively big equ...   \n705995          0                           Ate too much, feel sick    \n\n                                            cleaned_tweet  \n541200  My poor little dumpling In Holmdel vids he was...  \n750     I m off too bed I gotta wake up hella early to...  \n766711  I havent been able to listen to it yet My spea...  \n285055  now remembers why solving a relatively big equ...  \n705995                             Ate too much feel sick  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n      <th>cleaned_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>541200</th>\n      <td>0</td>\n      <td>@Nkluvr4eva My poor little dumpling  In Holmde...</td>\n      <td>My poor little dumpling In Holmdel vids he was...</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>0</td>\n      <td>I'm off too bed. I gotta wake up hella early t...</td>\n      <td>I m off too bed I gotta wake up hella early to...</td>\n    </tr>\n    <tr>\n      <th>766711</th>\n      <td>0</td>\n      <td>I havent been able to listen to it yet  My spe...</td>\n      <td>I havent been able to listen to it yet My spea...</td>\n    </tr>\n    <tr>\n      <th>285055</th>\n      <td>0</td>\n      <td>now remembers why solving a relatively big equ...</td>\n      <td>now remembers why solving a relatively big equ...</td>\n    </tr>\n    <tr>\n      <th>705995</th>\n      <td>0</td>\n      <td>Ate too much, feel sick</td>\n      <td>Ate too much feel sick</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train[data_train.sentiment>0].head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:56:23.277113Z","iopub.execute_input":"2024-05-05T11:56:23.277964Z","iopub.status.idle":"2024-05-05T11:56:23.303932Z","shell.execute_reply.started":"2024-05-05T11:56:23.277932Z","shell.execute_reply":"2024-05-05T11:56:23.303060Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"         sentiment                                              tweet  \\\n1189017          1            on lunch....dj should come eat with me    \n1365937          1  @TamaraSchilling Adventure - That's what we al...   \n1380170          1  @PerezHilton Zach makes me pee sitting down! A...   \n1233932          1  to sum up my day in one word ......... kackered!    \n1495500          1                    @k9wkj Great minds think alike    \n\n                                             cleaned_tweet  \n1189017                on lunch dj should come eat with me  \n1365937  Adventure That s what we all need in our life ...  \n1380170  Zach makes me pee sitting down And I m a grown...  \n1233932              to sum up my day in one word kackered  \n1495500                            Great minds think alike  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n      <th>cleaned_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1189017</th>\n      <td>1</td>\n      <td>on lunch....dj should come eat with me</td>\n      <td>on lunch dj should come eat with me</td>\n    </tr>\n    <tr>\n      <th>1365937</th>\n      <td>1</td>\n      <td>@TamaraSchilling Adventure - That's what we al...</td>\n      <td>Adventure That s what we all need in our life ...</td>\n    </tr>\n    <tr>\n      <th>1380170</th>\n      <td>1</td>\n      <td>@PerezHilton Zach makes me pee sitting down! A...</td>\n      <td>Zach makes me pee sitting down And I m a grown...</td>\n    </tr>\n    <tr>\n      <th>1233932</th>\n      <td>1</td>\n      <td>to sum up my day in one word ......... kackered!</td>\n      <td>to sum up my day in one word kackered</td>\n    </tr>\n    <tr>\n      <th>1495500</th>\n      <td>1</td>\n      <td>@k9wkj Great minds think alike</td>\n      <td>Great minds think alike</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, Trainer, TrainingArguments\nimport torch\n\n# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\nmodel = GPT2ForSequenceClassification.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:53:28.970571Z","iopub.execute_input":"2024-05-05T11:53:28.970960Z","iopub.status.idle":"2024-05-05T11:53:33.971448Z","shell.execute_reply.started":"2024-05-05T11:53:28.970933Z","shell.execute_reply":"2024-05-05T11:53:33.970639Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62b72120930476487083c17f6303dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec3af5ddad74b258ed05c80e7efec32"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87ae4ca5fd2437c8168fc7d17206a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55274a289ea242e3b7736bfe51593fee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d98929cd964bab8ceefa991dd2f095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfbc804f16a45e0b5271047092bcbb8"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel_name = \"llama\"\nnum_labels = 2 # replace with the actual number of labels in your classification task\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T14:07:38.599708Z","iopub.execute_input":"2024-05-05T14:07:38.600056Z","iopub.status.idle":"2024-05-05T14:07:45.874047Z","shell.execute_reply.started":"2024-05-05T14:07:38.600031Z","shell.execute_reply":"2024-05-05T14:07:45.872343Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/llama/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 393\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-663792b0-7f22d18b4fe52f4c3433de36;070a7982-d250-4796-a5f9-f2974e4c6669)\n\nRepository Not Found for url: https://huggingface.co/llama/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# replace with the actual number of labels in your classification task\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:484\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: llama is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"],"ename":"OSError","evalue":"llama is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","output_type":"error"}]},{"cell_type":"code","source":"# Prepare prompt and few-shot examples\nprompt = \"Sentiment classification: Positive tweets are labeled as 'positive', negative tweets are labeled as 'negative'. Please classify the sentiment of the following tweet:\"\npositive_example = \"I love this movie!\"\nnegative_example = \"This movie is terrible!\"","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:57:58.062030Z","iopub.execute_input":"2024-05-05T11:57:58.062916Z","iopub.status.idle":"2024-05-05T11:57:58.067258Z","shell.execute_reply.started":"2024-05-05T11:57:58.062883Z","shell.execute_reply":"2024-05-05T11:57:58.066137Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Prepare training data\ntrain_texts = [\"positive: \" + positive_example, \"negative: \" + negative_example]\ntrain_labels = [1, 0]  # 1 for positive, 0 for negative","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:57:59.178780Z","iopub.execute_input":"2024-05-05T11:57:59.179721Z","iopub.status.idle":"2024-05-05T11:57:59.184374Z","shell.execute_reply.started":"2024-05-05T11:57:59.179684Z","shell.execute_reply":"2024-05-05T11:57:59.183343Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}