{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-12T14:45:05.380897Z","iopub.execute_input":"2024-05-12T14:45:05.381158Z","iopub.status.idle":"2024-05-12T14:45:06.394563Z","shell.execute_reply.started":"2024-05-12T14:45:05.381133Z","shell.execute_reply":"2024-05-12T14:45:06.393480Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n/kaggle/input/sentiment140-bert/__results__.html\n/kaggle/input/sentiment140-bert/__notebook__.ipynb\n/kaggle/input/sentiment140-bert/__output__.json\n/kaggle/input/sentiment140-bert/custom.css\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:45:28.311281Z","iopub.execute_input":"2024-05-12T14:45:28.312336Z","iopub.status.idle":"2024-05-12T14:45:28.318120Z","shell.execute_reply.started":"2024-05-12T14:45:28.312301Z","shell.execute_reply":"2024-05-12T14:45:28.317058Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!pip install bert-for-tf2\n# !pip install sentencepiece\n# !pip install stormtrooper[setfit]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:45:56.322375Z","iopub.execute_input":"2024-05-12T14:45:56.323137Z","iopub.status.idle":"2024-05-12T14:46:17.022361Z","shell.execute_reply.started":"2024-05-12T14:45:56.323103Z","shell.execute_reply":"2024-05-12T14:46:17.021250Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting bert-for-tf2\n  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py-params>=0.9.6 (from bert-for-tf2)\n  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting params-flow>=0.8.0 (from bert-for-tf2)\n  Downloading params-flow-0.8.2.tar.gz (22 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.66.1)\nBuilding wheels for collected packages: bert-for-tf2, params-flow, py-params\n  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30509 sha256=26d5cab6e122ae2ec7e82cb12b82f2536cf586eae8b15b3696eb96abfe447a7b\n  Stored in directory: /root/.cache/pip/wheels/d8/da/50/126d7b8416d9a0e6bf876935c2219a71e72a6529c25e150c56\n  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19454 sha256=4e4bd7f5d3565b943b60cf6f2433dfd8bc757fb508f55dd3b790b863af91a3bb\n  Stored in directory: /root/.cache/pip/wheels/97/a8/d0/f7419404174976a2686bb98b5c30df01cc71445415f32db9e6\n  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7891 sha256=445f05a8bdc3b8d6b24f20978bb1e1c475e6fb2109a86789ff73b112d790a6d4\n  Stored in directory: /root/.cache/pip/wheels/69/c8/b3/92666cff9fb312bc3473eaa6b396695b89a7b3e31e90876819\nSuccessfully built bert-for-tf2 params-flow py-params\nInstalling collected packages: py-params, params-flow, bert-for-tf2\nSuccessfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport math\nfrom bs4 import BeautifulSoup  ## For cleanin the data\nimport random\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:46:40.993085Z","iopub.execute_input":"2024-05-12T14:46:40.993578Z","iopub.status.idle":"2024-05-12T14:46:41.025508Z","shell.execute_reply.started":"2024-05-12T14:46:40.993530Z","shell.execute_reply":"2024-05-12T14:46:41.024387Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Data Prep and Processing","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\ndata.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:46:43.508308Z","iopub.execute_input":"2024-05-12T14:46:43.509222Z","iopub.status.idle":"2024-05-12T14:46:58.922934Z","shell.execute_reply.started":"2024-05-12T14:46:43.509187Z","shell.execute_reply":"2024-05-12T14:46:58.921905Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   label        time                          date     query       username  \\\n0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n\n                                                text  \n0  is upset that he can't update his Facebook by ...  \n1  @Kenichan I dived many times for the ball. Man...  \n2    my whole body feels itchy and like its on fire   \n3  @nationwideclass no, it's not behaving at all....  \n4                      @Kwesidei not the whole crew   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>time</th>\n      <th>date</th>\n      <th>query</th>\n      <th>username</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811372</td>\n      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>joy_wolf</td>\n      <td>@Kwesidei not the whole crew</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:46:58.925206Z","iopub.execute_input":"2024-05-12T14:46:58.925559Z","iopub.status.idle":"2024-05-12T14:46:58.957923Z","shell.execute_reply.started":"2024-05-12T14:46:58.925529Z","shell.execute_reply":"2024-05-12T14:46:58.956812Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n4    800000\n0    799999\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data.drop([\"time\",\"date\",\"query\",\"username\"], axis= 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:47:23.115409Z","iopub.execute_input":"2024-05-12T14:47:23.116051Z","iopub.status.idle":"2024-05-12T14:47:23.165409Z","shell.execute_reply.started":"2024-05-12T14:47:23.116017Z","shell.execute_reply":"2024-05-12T14:47:23.164534Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data.columns=[ 'sentiment','tweet']\ndata['sentiment'] = data['sentiment'].replace(4,1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:47:23.875631Z","iopub.execute_input":"2024-05-12T14:47:23.876355Z","iopub.status.idle":"2024-05-12T14:47:23.900470Z","shell.execute_reply.started":"2024-05-12T14:47:23.876318Z","shell.execute_reply":"2024-05-12T14:47:23.899546Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   sentiment                                              tweet\n0          0  is upset that he can't update his Facebook by ...\n1          0  @Kenichan I dived many times for the ball. Man...\n2          0    my whole body feels itchy and like its on fire \n3          0  @nationwideclass no, it's not behaving at all....\n4          0                      @Kwesidei not the whole crew ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@Kwesidei not the whole crew</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Instead of working on complete data - create small fraction for test and train\n\n# data_train = data.sample(n=100000, random_state=42)\ndata_train = data.sample(frac=0.15, random_state=42)\nsubset2 = data.drop(data_train.index)\ndata_test = subset2.sample(frac=0.05, random_state=42)\n\nprint(f'New sampled train data:', {data_train.shape})\nprint(f'New sampled train data:', {data_test.shape})","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:47:25.816509Z","iopub.execute_input":"2024-05-12T14:47:25.817206Z","iopub.status.idle":"2024-05-12T14:47:26.086077Z","shell.execute_reply.started":"2024-05-12T14:47:25.817172Z","shell.execute_reply":"2024-05-12T14:47:26.085056Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"New sampled train data: {(240000, 2)}\nNew sampled train data: {(68000, 2)}\n","output_type":"stream"}]},{"cell_type":"code","source":"# data_train.to_csv('data_train.csv',index=False)\n# data_test.to_csv('data_test.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:49:59.746204Z","iopub.execute_input":"2024-05-12T14:49:59.747129Z","iopub.status.idle":"2024-05-12T14:50:00.035277Z","shell.execute_reply.started":"2024-05-12T14:49:59.747093Z","shell.execute_reply":"2024-05-12T14:50:00.034397Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data_train.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:50:23.146459Z","iopub.execute_input":"2024-05-12T14:50:23.147217Z","iopub.status.idle":"2024-05-12T14:50:23.156785Z","shell.execute_reply.started":"2024-05-12T14:50:23.147181Z","shell.execute_reply":"2024-05-12T14:50:23.155802Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"sentiment\n1    120175\n0    119825\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"data_test.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:50:23.621473Z","iopub.execute_input":"2024-05-12T14:50:23.621980Z","iopub.status.idle":"2024-05-12T14:50:23.629977Z","shell.execute_reply.started":"2024-05-12T14:50:23.621953Z","shell.execute_reply":"2024-05-12T14:50:23.628960Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"sentiment\n1    34082\n0    33918\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"def clean_tweet(tweet):\n    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n    tweet = re.sub(r\"https?://\\S+|www\\.\\S+\",\" \",tweet) # Removing URLS\n    tweet = re.sub(r\"@[A-Za-z0-9]+\",\" \",tweet) # @texts\n    tweet = re.sub(r\"https?://[A-Za-z0-9]+\",\" \",tweet) # Removing html tags\n    tweet = re.sub(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\",\" \",tweet) # Removing html tags\n    tweet = re.sub(r\"[^\\w\\s]\", \" \", tweet) # Removing Punctuation\n    tweet = re.sub(r\" +\", \" \", tweet) # Removing repeatition of anything\n    tweet = re.sub(r\"[^a-zA-Z.!?']\", \" \", tweet) # Removing non-letter words\n    tweet = re.sub(r\"\\s+\", \" \", tweet).strip() # Removing whitespace\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:50:25.444028Z","iopub.execute_input":"2024-05-12T14:50:25.444367Z","iopub.status.idle":"2024-05-12T14:50:25.451541Z","shell.execute_reply.started":"2024-05-12T14:50:25.444340Z","shell.execute_reply":"2024-05-12T14:50:25.450488Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"data_clean = [clean_tweet(tweet) for tweet in data_train.tweet] ## Cleaned Train data","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:50:26.052401Z","iopub.execute_input":"2024-05-12T14:50:26.052746Z","iopub.status.idle":"2024-05-12T14:51:17.343497Z","shell.execute_reply.started":"2024-05-12T14:50:26.052717Z","shell.execute_reply":"2024-05-12T14:51:17.342529Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4071081272.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n","output_type":"stream"}]},{"cell_type":"code","source":"data_test_clean = [clean_tweet(tweet) for tweet in data_test.tweet]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:51:17.344935Z","iopub.execute_input":"2024-05-12T14:51:17.345237Z","iopub.status.idle":"2024-05-12T14:51:31.739597Z","shell.execute_reply.started":"2024-05-12T14:51:17.345209Z","shell.execute_reply":"2024-05-12T14:51:31.738744Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4071081272.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"FullTokenization = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\", trainable = False)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\ntokenizer = FullTokenization(vocab_file,do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:51:42.244119Z","iopub.execute_input":"2024-05-12T14:51:42.245004Z","iopub.status.idle":"2024-05-12T14:52:01.289872Z","shell.execute_reply.started":"2024-05-12T14:51:42.244967Z","shell.execute_reply":"2024-05-12T14:52:01.289048Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.tokenize(\"This is such a weird place to LIVEEEE\")) ## Convert to token\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"This is such a weird place to LIVEEEE\"))) ## Token to IDS\ntokenizer.convert_ids_to_tokens([2023, 2003, 2107, 1037, 6881, 2173, 2000, 2444, 4402, 2063]) ## Is to tokens","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:52:13.815426Z","iopub.execute_input":"2024-05-12T14:52:13.815785Z","iopub.status.idle":"2024-05-12T14:52:13.824176Z","shell.execute_reply.started":"2024-05-12T14:52:13.815741Z","shell.execute_reply":"2024-05-12T14:52:13.823310Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"['this', 'is', 'such', 'a', 'weird', 'place', 'to', 'live', '##ee', '##e']\n[2023, 2003, 2107, 1037, 6881, 2173, 2000, 2444, 4402, 2063]\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['this', 'is', 'such', 'a', 'weird', 'place', 'to', 'live', '##ee', '##e']"},"metadata":{}}]},{"cell_type":"code","source":"## We will use first sentence  for BERT inputs -  CLS is for classification\ndef encode_sentence(sent):\n    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:52:15.757366Z","iopub.execute_input":"2024-05-12T14:52:15.758093Z","iopub.status.idle":"2024-05-12T14:52:15.762741Z","shell.execute_reply.started":"2024-05-12T14:52:15.758058Z","shell.execute_reply":"2024-05-12T14:52:15.761657Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"data_inputs = [encode_sentence(tweet) for tweet in data_clean]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:52:17.129888Z","iopub.execute_input":"2024-05-12T14:52:17.130820Z","iopub.status.idle":"2024-05-12T14:53:29.307128Z","shell.execute_reply.started":"2024-05-12T14:52:17.130784Z","shell.execute_reply":"2024-05-12T14:53:29.306257Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"data_inputs[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:53:29.308644Z","iopub.execute_input":"2024-05-12T14:53:29.308917Z","iopub.status.idle":"2024-05-12T14:53:29.315553Z","shell.execute_reply.started":"2024-05-12T14:53:29.308894Z","shell.execute_reply":"2024-05-12T14:53:29.314582Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'my',\n 'poor',\n 'little',\n 'dump',\n '##ling',\n 'in',\n 'holm',\n '##del',\n 'vi',\n '##ds',\n 'he',\n 'was',\n 'really',\n 'trying',\n 'hope',\n 'he',\n 'don',\n '##t',\n 'try',\n 'to',\n 'hard',\n 'tonight',\n 'xx',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Creation for BERT model\nWe need to create 3 different inputs  for each sentence","metadata":{}},{"cell_type":"code","source":"def get_ids(tokens):\n    return tokenizer.convert_tokens_to_ids(tokens)\n\n## Mask Padding\ndef get_mask(tokens):\n    return np.char.not_equal(tokens,\"[PAD]\").astype(int)\n\ndef get_segments(tokens):\n    seg_ids = []\n    current_seg_id = 0\n    for tok in tokens:\n        seg_ids.append(current_seg_id)\n        if  tok == '[SEP]':\n            current_seg_id = 1-current_seg_id\n    return seg_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:58:52.727585Z","iopub.execute_input":"2024-05-12T14:58:52.728017Z","iopub.status.idle":"2024-05-12T14:58:52.734891Z","shell.execute_reply.started":"2024-05-12T14:58:52.727962Z","shell.execute_reply":"2024-05-12T14:58:52.733791Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"data_labels = list(data_train.sentiment)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:58:54.627884Z","iopub.execute_input":"2024-05-12T14:58:54.628248Z","iopub.status.idle":"2024-05-12T14:58:54.660938Z","shell.execute_reply.started":"2024-05-12T14:58:54.628220Z","shell.execute_reply":"2024-05-12T14:58:54.659833Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"data_with_len = [[sent,data_labels[i],len(sent)] for i,sent in enumerate(data_inputs)]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:58:55.820057Z","iopub.execute_input":"2024-05-12T14:58:55.820418Z","iopub.status.idle":"2024-05-12T14:58:56.666968Z","shell.execute_reply.started":"2024-05-12T14:58:55.820389Z","shell.execute_reply":"2024-05-12T14:58:56.666054Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"data_with_len[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:58:56.668613Z","iopub.execute_input":"2024-05-12T14:58:56.668917Z","iopub.status.idle":"2024-05-12T14:58:56.676065Z","shell.execute_reply.started":"2024-05-12T14:58:56.668892Z","shell.execute_reply":"2024-05-12T14:58:56.675135Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"[['[CLS]',\n  'my',\n  'poor',\n  'little',\n  'dump',\n  '##ling',\n  'in',\n  'holm',\n  '##del',\n  'vi',\n  '##ds',\n  'he',\n  'was',\n  'really',\n  'trying',\n  'hope',\n  'he',\n  'don',\n  '##t',\n  'try',\n  'to',\n  'hard',\n  'tonight',\n  'xx',\n  '[SEP]'],\n 0,\n 25]"},"metadata":{}}]},{"cell_type":"code","source":"for i,sent_lab in enumerate(data_with_len):\n    if i<3:\n        if sent_lab[2]>7:\n            print(sent_lab)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:58:59.081883Z","iopub.execute_input":"2024-05-12T14:58:59.082832Z","iopub.status.idle":"2024-05-12T14:58:59.087802Z","shell.execute_reply.started":"2024-05-12T14:58:59.082790Z","shell.execute_reply":"2024-05-12T14:58:59.086916Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"[['[CLS]', 'my', 'poor', 'little', 'dump', '##ling', 'in', 'holm', '##del', 'vi', '##ds', 'he', 'was', 'really', 'trying', 'hope', 'he', 'don', '##t', 'try', 'to', 'hard', 'tonight', 'xx', '[SEP]'], 0, 25]\n","output_type":"stream"}]},{"cell_type":"code","source":"random.shuffle(data_with_len)\ndata_with_len.sort(key = lambda x: x[2])\n# sorted_all = [(sent_lab[0],sent_lab[1]) for sent_lab in data_with_len if sent_lab[2]>7]\n\nsorted_all = [([get_ids(sent_lab[0]), \n                get_mask(sent_lab[0]), \n                get_segments(sent_lab[0])], sent_lab[1])\n             for sent_lab in data_with_len if sent_lab[2]>7]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:59:00.001428Z","iopub.execute_input":"2024-05-12T14:59:00.001788Z","iopub.status.idle":"2024-05-12T14:59:06.484181Z","shell.execute_reply.started":"2024-05-12T14:59:00.001747Z","shell.execute_reply":"2024-05-12T14:59:06.483307Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all, \n                                              output_types = (tf.int32, tf.int32))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:59:06.485700Z","iopub.execute_input":"2024-05-12T14:59:06.486012Z","iopub.status.idle":"2024-05-12T14:59:06.523737Z","shell.execute_reply.started":"2024-05-12T14:59:06.485986Z","shell.execute_reply":"2024-05-12T14:59:06.522814Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"next(iter(all_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:59:06.524892Z","iopub.execute_input":"2024-05-12T14:59:06.525171Z","iopub.status.idle":"2024-05-12T14:59:06.586277Z","shell.execute_reply.started":"2024-05-12T14:59:06.525146Z","shell.execute_reply":"2024-05-12T14:59:06.585358Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(3, 8), dtype=int32, numpy=\n array([[ 101, 2298, 2378, 2070, 1043, 2094, 4790,  102],\n        [   1,    1,    1,    1,    1,    1,    1,    1],\n        [   0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)>,\n <tf.Tensor: shape=(), dtype=int32, numpy=1>)"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 32\n# all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes = ( (None, ), () ))\nall_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes = ( (3,None), () ))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T14:59:06.588266Z","iopub.execute_input":"2024-05-12T14:59:06.588538Z","iopub.status.idle":"2024-05-12T14:59:06.603194Z","shell.execute_reply.started":"2024-05-12T14:59:06.588514Z","shell.execute_reply":"2024-05-12T14:59:06.602414Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# next(iter(all_batched))","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:59:57.701357Z","iopub.execute_input":"2024-05-04T17:59:57.702191Z","iopub.status.idle":"2024-05-04T17:59:57.740874Z","shell.execute_reply.started":"2024-05-04T17:59:57.702158Z","shell.execute_reply":"2024-05-04T17:59:57.740003Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(32, 8), dtype=int32, numpy=\n array([[27571,  9541, 17139,  9152, 28954, 13900, 10474,  3122],\n        [ 7929,  2292,  1055,  2079,  2023,  2057, 19987,  2063],\n        [ 4283,  9212,  2072,  8840,  2140,  2200,  3835,  5449],\n        [ 2748,  2045,  2003,  1037,  2607,  2011,  2008,  2171],\n        [ 1045,  2123,  1056,  2228, 14106, 20627,  2030,  3571],\n        [ 2672,  1037,  2210,  4763,  2158,  2007,  1037,  8691],\n        [ 2129,  1045,  2777,  2115,  2388,  3084,  2033,  6517],\n        [ 1062,  1045,  4478,  1050,  2965,  9119, 23746,  8516],\n        [ 2039,  2025,  2061,  3835,  1998,  2220, 20323,  2082],\n        [ 4283,  2106,  2008,  2021,  2009,  2134,  1056,  2147],\n        [ 2123,  1056,  5390,  2009,  1055,  7929,  3081,  2078],\n        [ 4067,  2017,  2004, 17207,  2056,  1045,  2734,  2009],\n        [ 3666,  1996,  4564,  2059,  1996,  8692,  3185,  2982],\n        [23746,  2015,  2061, 10140,  1998,  2061,  2003,  2016],\n        [10047,  5962, 19810,  1045, 28667,  8462,  4859,  2017],\n        [ 7110,  1056, 17056,  4902,  1045,  1049,  3374, 13813],\n        [ 1045,  3984, 19902,  2003, 26478, 22974,  3560,  4656],\n        [ 2073,  1996,  6616,  2024,  2026,  2132, 19093,  6682],\n        [ 2467,  2245,  2009,  2001,  1996,  2060,  2126,  2105],\n        [ 3427,  8011,  2033,  2000,  3109,  2651,  3492,  2204],\n        [ 8117,  2187,  2000,  2147,  1998,  2059,  2733,  6209],\n        [ 2003,  5305,  2153,  1045,  5223,  2023, 13608,  4633],\n        [ 3599,  4165,  2066,  2017,  2128,  9107,  2115,  3944],\n        [ 1045, 20160,  2080,  2021, 22320,  2428,  3492,  2182],\n        [ 2758,  1037,  3148,  2860, 23848,  7170, 12835, 22110],\n        [ 2551,  2323,  2022,  7917,  2006,  2420,  2066,  2651],\n        [ 1045,  2113,  2008,  1055,  1996,  2028,  1045,  2215],\n        [ 2053,  2025,  1037,  3453,  1999,  2023,  2553, 22975],\n        [ 1045,  2074,  3427,  2008,  2792,  2153,  2197,  2305],\n        [ 2017,  2020,  5791,  4147,  2008,  6598,  2197,  2305],\n        [ 2204,  2851,  1045,  1049,  2145,  3103,  8022,  2102],\n        [ 3398,  4315,  3674,  2595,  2121, 14916,  3372,  9459]],\n       dtype=int32)>,\n <tf.Tensor: shape=(32,), dtype=int32, numpy=\n array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n        1, 1, 0, 0, 0, 0, 1, 1, 0, 1], dtype=int32)>)"},"metadata":{}}]},{"cell_type":"code","source":"# NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE)\n# NB_BATCHES_TEST = NB_BATCHES//10\n\n# all_batched.shuffle(NB_BATCHES)\n\n# ## Test and Train dataset create\n# test_dataset = all_batched.take(NB_BATCHES_TEST)\n# train_dataset = all_batched.skip(NB_BATCHES_TEST)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:06:02.446721Z","iopub.execute_input":"2024-05-04T18:06:02.447418Z","iopub.status.idle":"2024-05-04T18:06:02.466025Z","shell.execute_reply.started":"2024-05-04T18:06:02.447383Z","shell.execute_reply":"2024-05-04T18:06:02.465278Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# print(test_dataset)\n# train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-04T18:06:15.589542Z","iopub.execute_input":"2024-05-04T18:06:15.590276Z","iopub.status.idle":"2024-05-04T18:06:15.596932Z","shell.execute_reply.started":"2024-05-04T18:06:15.590242Z","shell.execute_reply":"2024-05-04T18:06:15.595985Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"<_TakeDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n","output_type":"stream"},{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"<_SkipDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model Train \n In BERT, we follow classic method\n  - Sentence Tokenization\n  - Sentence Embeddings -  Use pretrained model to do this.\n  - Build model - any CNN\n  ","metadata":{}},{"cell_type":"code","source":"class DCNN(tf.keras.Model):\n    \n    def __init__(self,\n                vocab_size,\n                emb_dim=128,\n                nb_filters = 50, ##filters in NN\n                FFN_units =512, ##Hidden layers\n                nb_classes = 2, ## Class count\n                dropout_rate= 0.1, ## for regularization\n                training= False,## Whether we want to train  or not\n                name = 'dcnn'): super(DCNN, self).__init__(name=name)\n        \n        self.embedding = layers.Embeddings(vocab_size, emb_dim)\n        self.bigram   = layers.Conv1D(filters= nb_filters, kernel_size = 2, padding = 'valid', activation = \"relu\")\n        \n        self.trigram  = layers.Conv1D(filters= nb_filters, kernel_size = 3, padding = 'valid', activation = \"relu\")\n        self.fourgram = layers.Conv1D(filters= nb_filters, kernel_size = 4, padding = 'valid', activation = \"relu\")\n        \n        self.pool = layers.GlobalAveragePooling1D()\n        \n        self.dense_1 = layers.Dense(units = FFN_units, activation = \"relu\")\n        self.dropout = layers.Dropout(rate = dropout_rate)\n        \n        if nb_classes = 2:\n            self.dense_2 = layers.Dense(units = 1, activation = \"relu\")\n        \n        \n        \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRY LLM classifier","metadata":{}},{"cell_type":"code","source":"data_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:45:28.529555Z","iopub.execute_input":"2024-05-05T11:45:28.529978Z","iopub.status.idle":"2024-05-05T11:45:28.540459Z","shell.execute_reply.started":"2024-05-05T11:45:28.529950Z","shell.execute_reply":"2024-05-05T11:45:28.539280Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"        sentiment                                              tweet\n541200          0  @Nkluvr4eva My poor little dumpling  In Holmde...\n750             0  I'm off too bed. I gotta wake up hella early t...\n766711          0  I havent been able to listen to it yet  My spe...\n285055          0  now remembers why solving a relatively big equ...\n705995          0                           Ate too much, feel sick ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>541200</th>\n      <td>0</td>\n      <td>@Nkluvr4eva My poor little dumpling  In Holmde...</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>0</td>\n      <td>I'm off too bed. I gotta wake up hella early t...</td>\n    </tr>\n    <tr>\n      <th>766711</th>\n      <td>0</td>\n      <td>I havent been able to listen to it yet  My spe...</td>\n    </tr>\n    <tr>\n      <th>285055</th>\n      <td>0</td>\n      <td>now remembers why solving a relatively big equ...</td>\n    </tr>\n    <tr>\n      <th>705995</th>\n      <td>0</td>\n      <td>Ate too much, feel sick</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train['cleaned_tweet'] = [clean_tweet(tweet) for tweet in data_train.tweet] ## Cleaned Train data\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:51:01.733101Z","iopub.execute_input":"2024-05-05T11:51:01.733467Z","iopub.status.idle":"2024-05-05T11:51:01.746650Z","shell.execute_reply.started":"2024-05-05T11:51:01.733440Z","shell.execute_reply":"2024-05-05T11:51:01.745608Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"        sentiment                                              tweet  \\\n541200          0  @Nkluvr4eva My poor little dumpling  In Holmde...   \n750             0  I'm off too bed. I gotta wake up hella early t...   \n766711          0  I havent been able to listen to it yet  My spe...   \n285055          0  now remembers why solving a relatively big equ...   \n705995          0                           Ate too much, feel sick    \n\n                                            cleaned_tweet  \n541200  My poor little dumpling In Holmdel vids he was...  \n750     I m off too bed I gotta wake up hella early to...  \n766711  I havent been able to listen to it yet My spea...  \n285055  now remembers why solving a relatively big equ...  \n705995                             Ate too much feel sick  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n      <th>cleaned_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>541200</th>\n      <td>0</td>\n      <td>@Nkluvr4eva My poor little dumpling  In Holmde...</td>\n      <td>My poor little dumpling In Holmdel vids he was...</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>0</td>\n      <td>I'm off too bed. I gotta wake up hella early t...</td>\n      <td>I m off too bed I gotta wake up hella early to...</td>\n    </tr>\n    <tr>\n      <th>766711</th>\n      <td>0</td>\n      <td>I havent been able to listen to it yet  My spe...</td>\n      <td>I havent been able to listen to it yet My spea...</td>\n    </tr>\n    <tr>\n      <th>285055</th>\n      <td>0</td>\n      <td>now remembers why solving a relatively big equ...</td>\n      <td>now remembers why solving a relatively big equ...</td>\n    </tr>\n    <tr>\n      <th>705995</th>\n      <td>0</td>\n      <td>Ate too much, feel sick</td>\n      <td>Ate too much feel sick</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train[data_train.sentiment>0].head()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:56:23.277113Z","iopub.execute_input":"2024-05-05T11:56:23.277964Z","iopub.status.idle":"2024-05-05T11:56:23.303932Z","shell.execute_reply.started":"2024-05-05T11:56:23.277932Z","shell.execute_reply":"2024-05-05T11:56:23.303060Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"         sentiment                                              tweet  \\\n1189017          1            on lunch....dj should come eat with me    \n1365937          1  @TamaraSchilling Adventure - That's what we al...   \n1380170          1  @PerezHilton Zach makes me pee sitting down! A...   \n1233932          1  to sum up my day in one word ......... kackered!    \n1495500          1                    @k9wkj Great minds think alike    \n\n                                             cleaned_tweet  \n1189017                on lunch dj should come eat with me  \n1365937  Adventure That s what we all need in our life ...  \n1380170  Zach makes me pee sitting down And I m a grown...  \n1233932              to sum up my day in one word kackered  \n1495500                            Great minds think alike  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet</th>\n      <th>cleaned_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1189017</th>\n      <td>1</td>\n      <td>on lunch....dj should come eat with me</td>\n      <td>on lunch dj should come eat with me</td>\n    </tr>\n    <tr>\n      <th>1365937</th>\n      <td>1</td>\n      <td>@TamaraSchilling Adventure - That's what we al...</td>\n      <td>Adventure That s what we all need in our life ...</td>\n    </tr>\n    <tr>\n      <th>1380170</th>\n      <td>1</td>\n      <td>@PerezHilton Zach makes me pee sitting down! A...</td>\n      <td>Zach makes me pee sitting down And I m a grown...</td>\n    </tr>\n    <tr>\n      <th>1233932</th>\n      <td>1</td>\n      <td>to sum up my day in one word ......... kackered!</td>\n      <td>to sum up my day in one word kackered</td>\n    </tr>\n    <tr>\n      <th>1495500</th>\n      <td>1</td>\n      <td>@k9wkj Great minds think alike</td>\n      <td>Great minds think alike</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, Trainer, TrainingArguments\nimport torch\n\n# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\nmodel = GPT2ForSequenceClassification.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:53:28.970571Z","iopub.execute_input":"2024-05-05T11:53:28.970960Z","iopub.status.idle":"2024-05-05T11:53:33.971448Z","shell.execute_reply.started":"2024-05-05T11:53:28.970933Z","shell.execute_reply":"2024-05-05T11:53:33.970639Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62b72120930476487083c17f6303dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec3af5ddad74b258ed05c80e7efec32"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87ae4ca5fd2437c8168fc7d17206a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55274a289ea242e3b7736bfe51593fee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d98929cd964bab8ceefa991dd2f095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfbc804f16a45e0b5271047092bcbb8"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel_name = \"llama\"\nnum_labels = 2 # replace with the actual number of labels in your classification task\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T14:07:38.599708Z","iopub.execute_input":"2024-05-05T14:07:38.600056Z","iopub.status.idle":"2024-05-05T14:07:45.874047Z","shell.execute_reply.started":"2024-05-05T14:07:38.600031Z","shell.execute_reply":"2024-05-05T14:07:45.872343Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/llama/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 393\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-663792b0-7f22d18b4fe52f4c3433de36;070a7982-d250-4796-a5f9-f2974e4c6669)\n\nRepository Not Found for url: https://huggingface.co/llama/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# replace with the actual number of labels in your classification task\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:484\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mOSError\u001b[0m: llama is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"],"ename":"OSError","evalue":"llama is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","output_type":"error"}]},{"cell_type":"code","source":"# Prepare prompt and few-shot examples\nprompt = \"Sentiment classification: Positive tweets are labeled as 'positive', negative tweets are labeled as 'negative'. Please classify the sentiment of the following tweet:\"\npositive_example = \"I love this movie!\"\nnegative_example = \"This movie is terrible!\"","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:57:58.062030Z","iopub.execute_input":"2024-05-05T11:57:58.062916Z","iopub.status.idle":"2024-05-05T11:57:58.067258Z","shell.execute_reply.started":"2024-05-05T11:57:58.062883Z","shell.execute_reply":"2024-05-05T11:57:58.066137Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Prepare training data\ntrain_texts = [\"positive: \" + positive_example, \"negative: \" + negative_example]\ntrain_labels = [1, 0]  # 1 for positive, 0 for negative","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:57:59.178780Z","iopub.execute_input":"2024-05-05T11:57:59.179721Z","iopub.status.idle":"2024-05-05T11:57:59.184374Z","shell.execute_reply.started":"2024-05-05T11:57:59.179684Z","shell.execute_reply":"2024-05-05T11:57:59.183343Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}